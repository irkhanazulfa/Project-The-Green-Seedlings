# -*- coding: utf-8 -*-
"""Project The Green Seedling.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12WHUKF5VBODWt_Uy3TYXZO1mYg8A-aij

# **BRI**

---

# **SCRAPING BRI**

### PLAYSTORE
"""

# install package
!pip install google-play-scraper
!pip install app_store_scraper

# Import all library
from google_play_scraper import Sort, reviews_all
from app_store_scraper import AppStore
import pandas as pd
import numpy as np
from google.colab import files
from datetime import date

from google_play_scraper import Sort, reviews_all
import pandas as pd
from datetime import datetime

# Define the search keywords
keywords = [
    "cybersecurity", "siber", "keamanan", "hacking", "fraud", "protection", "breach","kebocoran", "ancaman",
    "serangan", "phishing", "malware", "vulnerabilitas", "insiden", "pencurian", "rekening dibobol", "rekening hack",
    "rekening diretas", "transaksi tidak sah", "transaksi mencurigakan", "transaksi tidak dikenal", "saldo hilang",
    "uang raib", "uang hilang", "saldo berkurang", "penipuan online", "pelanggaran keamanan", "akun dibobol",
    "kejahatan siber", "pelanggaran keamanan", "kecurangan", "serangan", "cyber attack", "skimming", "pembobolan",
    "identity theft", "financial loss", "unauthorized access", "data leak", "account theft", "account takeover",
    "digital fraud", "data privasi", "data pribadi", "identitas", "serangan siber"
]

# Define the date range
start_date = datetime(2023, 6, 30)
end_date = datetime(2024, 6, 30)

# Scrape all reviews for the app
reviews = reviews_all(
    'id.co.bri.brimo',
    sleep_milliseconds=0,  # Adjust this value if needed to avoid overloading the server
    lang='id',  # Set the language to Indonesian
    country='id',  # Set the country to Indonesia
    sort=Sort.NEWEST  # Sort the reviews by newest first
)

# Convert to DataFrame
df = pd.DataFrame(reviews)

# Convert the date field to datetime
df['at'] = pd.to_datetime(df['at'])

# Filter reviews based on keywords
keyword_pattern = '|'.join(keywords)
filtered_by_keywords = df[df['content'].str.contains(keyword_pattern, case=False, na=False)]

# Filter reviews based on date range
filtered_reviews = filtered_by_keywords[(filtered_by_keywords['at'] >= start_date) & (filtered_by_keywords['at'] <= end_date)]

print(filtered_reviews.shape)
print(filtered_reviews.head())

# Save the DataFrame to a CSV file
filtered_reviews.to_csv('brimo_playstore.csv', encoding='utf-8-sig')

# Download the CSV file
files.download('brimo_playstore.csv')

"""# **PROSES**

### cek sentimen
"""

df = pd.read_csv('/content/brimo_playstore.csv')
# drop kolom yang engga penting
df.drop(columns={'Unnamed: 0','reviewId', 'userName', 'userImage', 'thumbsUpCount', 'reviewCreatedVersion', 'replyContent','repliedAt','appVersion'},inplace = True)
df.rename(columns= {'score': 'rating', 'content': 'review', 'at': 'date'},inplace = True)
df.insert(loc=0, column='source', value='Google Play')

df['source'].value_counts()['Google Play']

print('sentimen positif:', df[df.rating > 3].shape[0])
print('sentimen netral:', df[df.rating == 3].shape[0])
print('sentimen negatif:', df[df.rating < 3].shape[0])

"""### import package"""

# install package
!pip install Sastrawi

# Import library
import re
import nltk
import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from wordcloud import WordCloud
from collections import Counter
from string import punctuation
from IPython.display import Math

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

!pip install Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

!pip install jcopml
from jcopml.tuning import random_search_params as rsp

"""## Pre-processing data

### Create Text Processing Function
"""

def cleaner(sentence):
    review = sentence.lower()

    # clear double letters
    review = re.compile(r"(.)\1{1,}").sub(r"\1", review )
    # clear @username
    review = re.sub('@[^\s]+', '', review )
    # clear #tag
    review = re.sub(r'#([^\s]+)', '', review )
    # clear non-ascii value
    review = re.sub(r'[^\x00-\x7f]', r'', review )
    review = re.sub(r'(\\u[0-9A-Fa-f]+)', r'', review )
    review = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", review )
    review = re.sub(r'\\u\w\w\w\w', '', review )
    # remove symbol, number, and strange char
    review = re.sub(r"[.,:;+!\-_<^/=?\"'\(\)\d\*]", " ", review )

    return review

def slang_changer(review):
    kamus_slangword = eval(open("slangwords2.txt").read())
    # search pola kata (contoh kpn -> kapan)
    pattern = re.compile(r'\b( ' + '|'.join(kamus_slangword.keys()) + r')\b')
    content = []
    for kata in review:
        # replace slangword berdasarkan pola review yg telah ditentukan
        filteredSlang = pattern.sub(lambda x: kamus_slangword[x.group()], kata)
        content.append(filteredSlang.lower())
    review = content
    return review

# Membuat set stopwords dari NLTK dan punctuation
sw_indo = set(stopwords.words('indonesian') + list(punctuation))

# Menambahkan stopwords dari stop_words dan stopwords tambahan sendiri
additional_stopwords = ["kata1", "kata2", "kata3"]
sw_indo.update(word for word in StopWordRemoverFactory().get_stop_words() if word not in sw_indo)
sw_indo.update(additional_stopwords)

sw_indo = stopwords.words('indonesian') + list(punctuation)
sw_indo = sw_indo + [i for i in StopWordRemoverFactory().get_stop_words() if i not in sw_indo]
print(len(sw_indo))

"""### Clean Distraction Letters"""

df['clean'] = df.review.apply(lambda x: cleaner(x))

df.head()

"""### Tokenize"""

df['tokenize'] = df.clean.apply(lambda x: word_tokenize(x))

df.head()

"""### Change slangword"""

df['slangwords'] = df.tokenize.apply(lambda x: slang_changer(x))

df.head()

"""### Inverse tokenize"""

df['invers'] = df.slangwords.apply(lambda x: ' '.join(x))

df.head()

print('sentimen positif:', df[df.sentiment == 'positive'].shape[0])
print('sentimen netral:', df[df.sentiment == 'neutral'].shape[0])
print('sentimen negatif:', df[df.sentiment == 'negative'].shape[0])

from google.colab import files
df.to_csv('brimo_done.csv', index=False)
files.download('brimo_done.csv')

"""## Translate"""

!pip install googletrans==3.1.0a0

import pandas as pd
import googletrans
from googletrans import Translator

df_new = df[df["rating"] >= 3]
df_new.shape

translator = Translator()
df_new['invers'] = df_new['invers'].astype(str) #changing datatype to string
df_new['review_eng'] = df_new['invers'].apply(lambda x:translator.translate(x).text)
df_new

"""## Labeling"""

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

score = [analyzer.polarity_scores(x) for x in df_new['review_eng']]
df_new['negative_score'] = [x['neg'] for x in score]
df_new['neutral_score'] = [x['neu'] for x in score]
df_new['positive_score'] = [x['pos'] for x in score]
df_new['compound_score'] = [x['compound'] for x in score]

df_new['sentiment'] = df_new.compound_score.apply(lambda x: 'positive' if x>=0.05 else ('negative' if x<=-0.05 else 'neutral'))

df_new.head()

print('sentimen positive:', df_new[df_new.sentiment == 'positive'].shape[0])
print('sentimen neutral:', df_new[df_new.sentiment == 'neutral'].shape[0])
print('sentimen negative:', df_new[df_new.sentiment == 'negative'].shape[0])

# Save the result
from google.colab import files
df_new.to_csv('brimo_done2.csv', index=False)
files.download('brimo_done2.csv')

"""## concat review"""

df_new2 = df[df["rating"] < 3]
df_new2.shape

df_new2['sentiment'] = df_new2.rating.apply(lambda x: 'neutral' if x==3 else 'negative')

df_result = pd.concat([df_new, df_new2], sort=False).sort_index()
df_result.head()

print('sentimen positive:', df_result[df_result.sentiment == 'positive'].shape[0])
print('sentimen neutral:', df_result[df_result.sentiment == 'neutral'].shape[0])
print('sentimen negative:', df_result[df_result.sentiment == 'negative'].shape[0])

# Save the result
from google.colab import files
df_result.to_csv('brimo_done3.csv', index=False)
files.download('brimo_done3.csv')

"""## Pre-processing (2)"""

# Importing dataset
import pandas as pd
df = pd.read_csv('brimo_done3.csv')

# create function
def stopword_changer(sentence):
    return [i for i in sentence if i not in sw_indo]

def stemmer_changer(sentence):
  return [StemmerFactory().create_stemmer().stem(i) for i in sentence]

df.review

df['tokenize2'] = df.invers.apply(lambda x: word_tokenize(str(x)) if pd.notnull(x) else [])

# delete stopword
df['stopword'] = df.tokenize2.apply(lambda x: stopword_changer(x))

df.stopword

# stemming
df['stemming'] = df.stopword.apply(lambda x: stemmer_changer(x))

# invers token
df['invers2'] = df.stemming.apply(lambda x: ' '.join(x))

# save
from google.colab import files
df.to_csv('brimo_ready.csv', index=False)
files.download('brimo_ready.csv')

"""# **Exploratory Data Analysis**"""

df = pd.read_csv('/content/brimo_ready.csv')

"""### rasio sentimen"""

# install package
import numpy as np
import pandas as pd
# For visualizations
import matplotlib.pyplot as plt
# For regular expressions
import re
# For handling string
import string
# For performing mathematical operations
import math

print('sentimen positif:', df[df.sentiment == 'positive'].shape[0])
print('sentimen netral:', df[df.sentiment == 'neutral'].shape[0])
print('sentimen negatif:', df[df.sentiment == 'negative'].shape[0])

import matplotlib.pyplot as plt

# Setting labels
labels = ['NEGATIVE', 'POSITIVE', 'NEUTRAL']

# Setting value
# Add the count for the 'neutral' sentiment
data = [
    df[df.sentiment == 'negative'].shape[0],
    df[df.sentiment == 'positive'].shape[0],
    df[df.sentiment == 'neutral'].shape[0]
]

# colors
colors = ['#FD7702', '#003366', '#808080']  # Added color for neutral

# explosion
explode = (0.05, 0.05, 0.05)  # Added explode for neutral

# Pie Chart
plt.pie(data, colors=colors, labels=labels,
        autopct='%1.1f%%', pctdistance=0.85, explode=explode)

# draw circle
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()

# Adding Circle in Pie chart
fig.gca().add_artist(centre_circle)

# Adding Title of chart
plt.title('SENTIMEN ANALYSIS')

# Displaying Chart
plt.show()

"""### sentimen overtime"""

import pandas as pd
import matplotlib.pyplot as plt

# Misalkan df adalah DataFrame Anda
# Pastikan kolom 'date' sudah dalam format datetime
df['date'] = pd.to_datetime(df['date'])

# Ekstrak bulan dari kolom 'date'
df['month'] = df['date'].dt.to_period('M')

# Kelompokkan data berdasarkan bulan dan sentimen
grouped_data = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)

# Gambarkan grafik garis
plt.figure(figsize=(20, 8))
ax = grouped_data[['positive', 'negative', 'neutral']].plot(kind='line', marker='o', linestyle='-', color=['#003366', '#FD7702', '#808080'])

# Menambahkan label pada setiap titik data
for sentiment in ['positive', 'negative', 'neutral']:
    for i, value in enumerate(grouped_data[sentiment]):
        ax.annotate(f'{value}',  # Label value
                    (grouped_data.index[i], value),  # Point location
                    textcoords="offset points",
                    xytext=(0,5),  # Offset position
                    ha='center',
                    fontsize=8,  # Font size for clarity
                    bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))  # Background for label

# Konfigurasi tambahan untuk plot
plt.title('Sentiment Over Time')
plt.xlabel('Month')
plt.ylabel('Number of Reviews')
plt.legend(title='Sentiment', loc='upper right')

# Menampilkan grafik
plt.tight_layout()  # Adjust layout to ensure everything fits
plt.show()

import pandas as pd

# Misalkan df adalah DataFrame Anda
# Pastikan kolom 'date' sudah dalam format datetime
df['date'] = pd.to_datetime(df['date'])

# Ekstrak bulan dari kolom 'date'
df['month'] = df['date'].dt.to_period('M')

# Kelompokkan data berdasarkan bulan dan sentimen
grouped_data = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)

# Simpan tabel ke dalam file CSV
grouped_data.to_csv('sentiment_per_month.csv', index=True)

print("Tabel berhasil disimpan ke dalam file sentiment_per_month.csv")

"""## Topic modelling

### TF-IDF
"""

import pandas as pd
Data = pd.read_csv('/content/brimo_ready.csv')

from gensim import corpora
from gensim.models import TfidfModel, LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Praproses teks
documents = Data.loc[Data['sentiment'] == 'negative', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus_bow = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model TF-IDF
tfidf_model = TfidfModel(corpus_bow)
corpus_tfidf = tfidf_model[corpus_bow]

# Membangun model LDA dengan representasi TF-IDF
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

"""### BOW"""

from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Preproses teks
documents = Data.loc[Data['sentiment'] == 'negative', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model LDA
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

"""## Top word"""

Data['sentiment'].value_counts()

Data_All = Data['invers2']
Data_Negatif = Data[Data['sentiment'] == 'negative']['invers2']
Data_Netral = Data[Data['sentiment'] == 'neutral']['invers2']
Data_Positif = Data[Data['sentiment'] == 'positive']['invers2']

# Python program to convert a list
# using list comprehension
data_split_all = ' '.join(map(str, Data_All))
data_split_negatif = ' '.join(map(str, Data_Negatif))
data_split_positif = ' '.join(map(str, Data_Positif))

# split() returns list of all the words in the string
split_it_all = data_split_all.split()
split_it_negatif = data_split_negatif.split()
split_it_positif = data_split_positif.split()

# Pass the split_it list to instance of Counter class.
Counter_all = Counter(split_it_all)
Counter_negatif = Counter(split_it_negatif)
Counter_positif = Counter(split_it_positif)

# most_common() produces k frequently encountered
# input values and their respective counts.
most_occur_all = Counter_all.most_common(10)
most_occur_negatif = Counter_negatif.most_common(10)
most_occur_positif = Counter_positif.most_common(10)

print(most_occur_all)
print(len(Counter_all))
print(most_occur_negatif)
print(len(Counter_negatif))
print(most_occur_positif)
print(len(Counter_positif))

# Sort words and frequencies for all data in ascending order
sorted_all = sorted(most_occur_all, key=lambda x: x[1])
words_all, frequencies_all = zip(*sorted_all)

# Create a horizontal bar chart for all data
plt.figure(figsize=(10, 6))
plt.barh(words_all, frequencies_all, color='blue')
plt.title('Top Words in All Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

# Sort words and frequencies for negative data in ascending order
sorted_negatif = sorted(most_occur_negatif, key=lambda x: x[1])
words_negatif, frequencies_negatif = zip(*sorted_negatif)

# Create a horizontal bar chart for negative data
plt.figure(figsize=(10, 6))
plt.barh(words_negatif, frequencies_negatif, color='red')
plt.title('Top Words in Negative Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

# Sort words and frequencies for positive data in ascending order
sorted_positif = sorted(most_occur_positif, key=lambda x: x[1])
words_positif, frequencies_positif = zip(*sorted_positif)

# Create a horizontal bar chart for positive data
plt.figure(figsize=(10, 6))
plt.barh(words_positif, frequencies_positif, color='green')
plt.title('Top Words in Positive Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

"""## Word Cloud"""

df_positive = Data.copy()[Data.sentiment == 'positive']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				colormap='seismic',
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_positive.png')
plt.show()

df_positive = Data.copy()[Data.sentiment == 'negative']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				colormap = 'seismic',
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

df_positive = Data.copy()[Data.sentiment == 'neutral']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

"""# **BCA**

---

# **SCRAPING BCA**

### PLAYSTORE
"""

# install package
!pip install google-play-scraper
!pip install app_store_scraper

# Import all library
from google_play_scraper import Sort, reviews_all
from app_store_scraper import AppStore
import pandas as pd
import numpy as np
from google.colab import files
from datetime import date

from google_play_scraper import Sort, reviews_all
import pandas as pd
from datetime import datetime

# Define the search keywords
keywords = [
    "cybersecurity", "siber", "keamanan", "hacking", "fraud", "protection", "breach","kebocoran", "ancaman",
    "serangan", "phishing", "malware", "vulnerabilitas", "insiden", "pencurian", "rekening dibobol", "rekening hack",
    "rekening diretas", "transaksi tidak sah", "transaksi mencurigakan", "transaksi tidak dikenal", "saldo hilang",
    "uang raib", "uang hilang", "saldo berkurang", "penipuan online", "pelanggaran keamanan", "akun dibobol",
    "kejahatan siber", "pelanggaran keamanan", "kecurangan", "serangan", "cyber attack", "skimming", "pembobolan",
    "identity theft", "financial loss", "unauthorized access", "data leak", "account theft", "account takeover",
    "digital fraud", "data privasi", "data pribadi", "identitas", "serangan siber"
]

# Define the date range
start_date = datetime(2023, 6, 30)
end_date = datetime(2024, 6, 30)

# Scrape all reviews for the app
reviews = reviews_all(
    'com.bca',
    sleep_milliseconds=0,  # Adjust this value if needed to avoid overloading the server
    lang='id',  # Set the language to Indonesian
    country='id',  # Set the country to Indonesia
    sort=Sort.NEWEST  # Sort the reviews by newest first
)

# Convert to DataFrame
df = pd.DataFrame(reviews)

# Convert the date field to datetime
df['at'] = pd.to_datetime(df['at'])

# Filter reviews based on keywords
keyword_pattern = '|'.join(keywords)
filtered_by_keywords = df[df['content'].str.contains(keyword_pattern, case=False, na=False)]

# Filter reviews based on date range
filtered_reviews = filtered_by_keywords[(filtered_by_keywords['at'] >= start_date) & (filtered_by_keywords['at'] <= end_date)]

print(filtered_reviews.shape)
print(filtered_reviews.head())

# Save the DataFrame to a CSV file
filtered_reviews.to_csv('bca_playstore.csv', encoding='utf-8-sig')

# Download the CSV file
files.download('bca_playstore.csv')

"""# **PROSES**

### cek sentimen
"""

df = pd.read_csv('/content/bca_playstore.csv')
# drop kolom yang engga penting
df.drop(columns={'Unnamed: 0','reviewId', 'userName', 'userImage', 'thumbsUpCount', 'reviewCreatedVersion', 'replyContent','repliedAt','appVersion'},inplace = True)
df.rename(columns= {'score': 'rating', 'content': 'review', 'at': 'date'},inplace = True)
df.insert(loc=0, column='source', value='Google Play')

df['source'].value_counts()['Google Play']

print('sentimen positif:', df[df.rating > 3].shape[0])
print('sentimen netral:', df[df.rating == 3].shape[0])
print('sentimen negatif:', df[df.rating < 3].shape[0])

"""### import package"""

# install package
!pip install Sastrawi

# Import library
import re
import nltk
import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from wordcloud import WordCloud
from collections import Counter
from string import punctuation
from IPython.display import Math

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

!pip install Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

!pip install jcopml
from jcopml.tuning import random_search_params as rsp

"""## Pre-processing data

### Create Text Processing Function
"""

def cleaner(sentence):
    review = sentence.lower()

    # clear double letters
    review = re.compile(r"(.)\1{1,}").sub(r"\1", review )
    # clear @username
    review = re.sub('@[^\s]+', '', review )
    # clear #tag
    review = re.sub(r'#([^\s]+)', '', review )
    # clear non-ascii value
    review = re.sub(r'[^\x00-\x7f]', r'', review )
    review = re.sub(r'(\\u[0-9A-Fa-f]+)', r'', review )
    review = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", review )
    review = re.sub(r'\\u\w\w\w\w', '', review )
    # remove symbol, number, and strange char
    review = re.sub(r"[.,:;+!\-_<^/=?\"'\(\)\d\*]", " ", review )

    return review

def slang_changer(review):
    kamus_slangword = eval(open("slangwords2.txt").read())
    # search pola kata (contoh kpn -> kapan)
    pattern = re.compile(r'\b( ' + '|'.join(kamus_slangword.keys()) + r')\b')
    content = []
    for kata in review:
        # replace slangword berdasarkan pola review yg telah ditentukan
        filteredSlang = pattern.sub(lambda x: kamus_slangword[x.group()], kata)
        content.append(filteredSlang.lower())
    review = content
    return review

# Membuat set stopwords dari NLTK dan punctuation
sw_indo = set(stopwords.words('indonesian') + list(punctuation))

# Menambahkan stopwords dari stop_words dan stopwords tambahan sendiri
additional_stopwords = ["kata1", "kata2", "kata3"]
sw_indo.update(word for word in StopWordRemoverFactory().get_stop_words() if word not in sw_indo)
sw_indo.update(additional_stopwords)

sw_indo = stopwords.words('indonesian') + list(punctuation)
sw_indo = sw_indo + [i for i in StopWordRemoverFactory().get_stop_words() if i not in sw_indo]
print(len(sw_indo))

"""### Clean Distraction Letters"""

df['clean'] = df.review.apply(lambda x: cleaner(x))

df.head()

"""### Tokenize"""

df['tokenize'] = df.clean.apply(lambda x: word_tokenize(x))

df.head()

"""### Change slangword"""

df['slangwords'] = df.tokenize.apply(lambda x: slang_changer(x))

df.head()

"""### Inverse tokenize"""

df['invers'] = df.slangwords.apply(lambda x: ' '.join(x))

df.head()

from google.colab import files
df.to_csv('bca_done.csv', index=False)
files.download('bca_done.csv')

"""## Translate"""

!pip install googletrans==3.1.0a0

import pandas as pd
import googletrans
from googletrans import Translator

df_new = df[df["rating"] >= 3]
df_new.shape

translator = Translator()
df_new['invers'] = df_new['invers'].astype(str) #changing datatype to string
df_new['review_eng'] = df_new['invers'].apply(lambda x:translator.translate(x).text)
df_new

"""## Labeling"""

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

score = [analyzer.polarity_scores(x) for x in df_new['review_eng']]
df_new['negative_score'] = [x['neg'] for x in score]
df_new['neutral_score'] = [x['neu'] for x in score]
df_new['positive_score'] = [x['pos'] for x in score]
df_new['compound_score'] = [x['compound'] for x in score]

df_new['sentiment'] = df_new.compound_score.apply(lambda x: 'positive' if x>=0.05 else ('negative' if x<=-0.05 else 'neutral'))

df_new.head()

print('sentimen positive:', df_new[df_new.sentiment == 'positive'].shape[0])
print('sentimen neutral:', df_new[df_new.sentiment == 'neutral'].shape[0])
print('sentimen negative:', df_new[df_new.sentiment == 'negative'].shape[0])

# Save the result
from google.colab import files
df_new.to_csv('bca_done2.csv', index=False)
files.download('bca_done2.csv')

"""## concat review"""

df_new2 = df[df["rating"] < 3]
df_new.shape

df_new2['sentiment'] = df_new2.rating.apply(lambda x: 'neutral' if x==3 else 'negative')

df_result = pd.concat([df_new, df_new2], sort=False).sort_index()
df_result.head()

print('sentimen positive:', df_result[df_result.sentiment == 'positive'].shape[0])
print('sentimen neutral:', df_result[df_result.sentiment == 'neutral'].shape[0])
print('sentimen negative:', df_result[df_result.sentiment == 'negative'].shape[0])

# Save the result
from google.colab import files
df_result.to_csv('bca_done3.csv', index=False)
files.download('bca_done3.csv')

"""## Pre-processing (2)"""

# Importing dataset
import pandas as pd
df = pd.read_csv('bca_done3.csv')

# create function
def stopword_changer(sentence):
    return [i for i in sentence if i not in sw_indo]

def stemmer_changer(sentence):
  return [StemmerFactory().create_stemmer().stem(i) for i in sentence]

df.review

df['tokenize2'] = df.invers.apply(lambda x: word_tokenize(str(x)) if pd.notnull(x) else [])

# delete stopword
df['stopword'] = df.tokenize2.apply(lambda x: stopword_changer(x))

df.stopword

# stemming
df['stemming'] = df.stopword.apply(lambda x: stemmer_changer(x))

# invers token
df['invers2'] = df.stemming.apply(lambda x: ' '.join(x))

# save
from google.colab import files
df.to_csv('bca_ready.csv', index=False)
files.download('bca_ready.csv')

"""# **Exploratory Data Analysis**"""

df = pd.read_csv('/content/bca_ready.csv')

"""### rasio sentimen"""

# install package
import numpy as np
import pandas as pd
# For visualizations
import matplotlib.pyplot as plt
# For regular expressions
import re
# For handling string
import string
# For performing mathematical operations
import math

print('sentimen positif:', df[df.sentiment == 'positive'].shape[0])
print('sentimen netral:', df[df.sentiment == 'neutral'].shape[0])
print('sentimen negatif:', df[df.sentiment == 'negative'].shape[0])

import matplotlib.pyplot as plt

# Setting labels
labels = ['NEGATIVE', 'POSITIVE', 'NEUTRAL']

# Setting value
# Add the count for the 'neutral' sentiment
data = [
    df[df.sentiment == 'negative'].shape[0],
    df[df.sentiment == 'positive'].shape[0],
    df[df.sentiment == 'neutral'].shape[0]
]

# colors
colors = ['#FD7702', '#003366', '#808080']  # Added color for neutral

# explosion
explode = (0.05, 0.05, 0.05)  # Added explode for neutral

# Pie Chart
plt.pie(data, colors=colors, labels=labels,
        autopct='%1.1f%%', pctdistance=0.85, explode=explode)

# draw circle
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()

# Adding Circle in Pie chart
fig.gca().add_artist(centre_circle)

# Adding Title of chart
plt.title('SENTIMEN ANALYSIS')

# Displaying Chart
plt.show()

"""### sentimen overtime"""

import pandas as pd
import matplotlib.pyplot as plt

# Misalkan df adalah DataFrame Anda
# Pastikan kolom 'date' sudah dalam format datetime
df['date'] = pd.to_datetime(df['date'])

# Ekstrak bulan dari kolom 'date'
df['month'] = df['date'].dt.to_period('M')

# Kelompokkan data berdasarkan bulan dan sentimen
grouped_data = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)

# Gambarkan grafik garis
plt.figure(figsize=(20, 8))
ax = grouped_data[['positive', 'negative', 'neutral']].plot(kind='line', marker='o', linestyle='-', color=['#003366', '#FD7702', '#808080'])

# Menambahkan label pada setiap titik data
for sentiment in ['positive', 'negative', 'neutral']:
    for i, value in enumerate(grouped_data[sentiment]):
        ax.annotate(f'{value}',  # Label value
                    (grouped_data.index[i], value),  # Point location
                    textcoords="offset points",
                    xytext=(0,5),  # Offset position
                    ha='center',
                    fontsize=8,  # Font size for clarity
                    bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))  # Background for label

# Konfigurasi tambahan untuk plot
plt.title('Sentiment Over Time')
plt.xlabel('Month')
plt.ylabel('Number of Reviews')
plt.legend(title='Sentiment', loc='upper right')

# Menampilkan grafik
plt.tight_layout()  # Adjust layout to ensure everything fits
plt.show()

import pandas as pd

# Misalkan df adalah DataFrame Anda
# Pastikan kolom 'date' sudah dalam format datetime
df['date'] = pd.to_datetime(df['date'])

# Ekstrak bulan dari kolom 'date'
df['month'] = df['date'].dt.to_period('M')

# Kelompokkan data berdasarkan bulan dan sentimen
grouped_data = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)

# Simpan tabel ke dalam file CSV
grouped_data.to_csv('sentiment_per_monthbca.csv', index=True)

print("Tabel berhasil disimpan ke dalam file sentiment_per_month.csv")

"""## Topic modelling

### TF-IDF
"""

import pandas as pd
Data = pd.read_csv('/content/bca_ready.csv')

from gensim import corpora
from gensim.models import TfidfModel, LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Praproses teks
documents = Data.loc[Data['sentiment'] == 'negative', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus_bow = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model TF-IDF
tfidf_model = TfidfModel(corpus_bow)
corpus_tfidf = tfidf_model[corpus_bow]

# Membangun model LDA dengan representasi TF-IDF
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

"""### BOW"""

from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Preproses teks
documents = Data.loc[Data['sentiment'] == 'negative', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model LDA
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

"""## Top word"""

Data['sentiment'].value_counts()

Data_All = Data['invers2']
Data_Negatif = Data[Data['sentiment'] == 'negative']['invers2']
Data_Netral = Data[Data['sentiment'] == 'neutral']['invers2']
Data_Positif = Data[Data['sentiment'] == 'positive']['invers2']

# Python program to convert a list
# using list comprehension
data_split_all = ' '.join(map(str, Data_All))
data_split_negatif = ' '.join(map(str, Data_Negatif))
data_split_positif = ' '.join(map(str, Data_Positif))

# split() returns list of all the words in the string
split_it_all = data_split_all.split()
split_it_negatif = data_split_negatif.split()
split_it_positif = data_split_positif.split()

# Pass the split_it list to instance of Counter class.
Counter_all = Counter(split_it_all)
Counter_negatif = Counter(split_it_negatif)
Counter_positif = Counter(split_it_positif)

# most_common() produces k frequently encountered
# input values and their respective counts.
most_occur_all = Counter_all.most_common(10)
most_occur_negatif = Counter_negatif.most_common(10)
most_occur_positif = Counter_positif.most_common(10)

print(most_occur_all)
print(len(Counter_all))
print(most_occur_negatif)
print(len(Counter_negatif))
print(most_occur_positif)
print(len(Counter_positif))

# Sort words and frequencies for all data in ascending order
sorted_all = sorted(most_occur_all, key=lambda x: x[1])
words_all, frequencies_all = zip(*sorted_all)

# Create a horizontal bar chart for all data
plt.figure(figsize=(10, 6))
plt.barh(words_all, frequencies_all, color='blue')
plt.title('Top Words in All Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

# Sort words and frequencies for negative data in ascending order
sorted_negatif = sorted(most_occur_negatif, key=lambda x: x[1])
words_negatif, frequencies_negatif = zip(*sorted_negatif)

# Create a horizontal bar chart for negative data
plt.figure(figsize=(10, 6))
plt.barh(words_negatif, frequencies_negatif, color='red')
plt.title('Top Words in Negative Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

# Sort words and frequencies for positive data in ascending order
sorted_positif = sorted(most_occur_positif, key=lambda x: x[1])
words_positif, frequencies_positif = zip(*sorted_positif)

# Create a horizontal bar chart for positive data
plt.figure(figsize=(10, 6))
plt.barh(words_positif, frequencies_positif, color='green')
plt.title('Top Words in Positive Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

"""## Word Cloud"""

df_positive = Data.copy()[Data.sentiment == 'positive']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_positive.png')
plt.show()

df_positive = Data.copy()[Data.sentiment == 'negative']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

df_positive = Data.copy()[Data.sentiment == 'neutral']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

"""# **MANDIRI**

---

# **SCRAPING MANDIRI**

### PLAYSTORE
"""

# install package
!pip install google-play-scraper
!pip install app_store_scraper

# Import all library
from google_play_scraper import Sort, reviews_all
from app_store_scraper import AppStore
import pandas as pd
import numpy as np
from google.colab import files
from datetime import date

from google_play_scraper import Sort, reviews_all
import pandas as pd
from datetime import datetime

# Define the search keywords
keywords = [
    "cybersecurity", "siber", "keamanan", "hacking", "fraud", "protection", "breach","kebocoran", "ancaman",
    "serangan", "phishing", "malware", "vulnerabilitas", "insiden", "pencurian", "rekening dibobol", "rekening hack",
    "rekening diretas", "transaksi tidak sah", "transaksi mencurigakan", "transaksi tidak dikenal", "saldo hilang",
    "uang raib", "uang hilang", "saldo berkurang", "penipuan online", "pelanggaran keamanan", "akun dibobol",
    "kejahatan siber", "pelanggaran keamanan", "kecurangan", "serangan", "cyber attack", "skimming", "pembobolan",
    "identity theft", "financial loss", "unauthorized access", "data leak", "account theft", "account takeover",
    "digital fraud", "data privasi", "data pribadi", "identitas", "serangan siber"
]

# Define the date range
start_date = datetime(2023, 6, 30)
end_date = datetime(2024, 6, 30)

# Scrape all reviews for the app
reviews = reviews_all(
    'id.bmri.livin',
    sleep_milliseconds=0,  # Adjust this value if needed to avoid overloading the server
    lang='id',  # Set the language to Indonesian
    country='id',  # Set the country to Indonesia
    sort=Sort.NEWEST  # Sort the reviews by newest first
)

# Convert to DataFrame
df = pd.DataFrame(reviews)

# Convert the date field to datetime
df['at'] = pd.to_datetime(df['at'])

# Filter reviews based on keywords
keyword_pattern = '|'.join(keywords)
filtered_by_keywords = df[df['content'].str.contains(keyword_pattern, case=False, na=False)]

# Filter reviews based on date range
filtered_reviews = filtered_by_keywords[(filtered_by_keywords['at'] >= start_date) & (filtered_by_keywords['at'] <= end_date)]

print(filtered_reviews.shape)
print(filtered_reviews.head())

# Save the DataFrame to a CSV file
filtered_reviews.to_csv('mandiri_playstore.csv', encoding='utf-8-sig')

# Download the CSV file
files.download('mandiri_playstore.csv')

"""# **PROSES**

### cek sentimen
"""

df = pd.read_csv('/content/mandiri_playstore.csv')
# drop kolom yang engga penting
df.drop(columns={'Unnamed: 0','reviewId', 'userName', 'userImage', 'thumbsUpCount', 'reviewCreatedVersion', 'replyContent','repliedAt','appVersion'},inplace = True)
df.rename(columns= {'score': 'rating', 'content': 'review', 'at': 'date'},inplace = True)
df.insert(loc=0, column='source', value='Google Play')

df['source'].value_counts()['Google Play']

print('sentimen positif:', df[df.rating > 3].shape[0])
print('sentimen netral:', df[df.rating == 3].shape[0])
print('sentimen negatif:', df[df.rating < 3].shape[0])

"""### import package"""

# install package
!pip install Sastrawi

# Import library
import re
import nltk
import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from wordcloud import WordCloud
from collections import Counter
from string import punctuation
from IPython.display import Math

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

!pip install Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

!pip install jcopml
from jcopml.tuning import random_search_params as rsp

"""## Pre-processing data

### Create Text Processing Function
"""

def cleaner(sentence):
    review = sentence.lower()

    # clear double letters
    review = re.compile(r"(.)\1{1,}").sub(r"\1", review )
    # clear @username
    review = re.sub('@[^\s]+', '', review )
    # clear #tag
    review = re.sub(r'#([^\s]+)', '', review )
    # clear non-ascii value
    review = re.sub(r'[^\x00-\x7f]', r'', review )
    review = re.sub(r'(\\u[0-9A-Fa-f]+)', r'', review )
    review = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", review )
    review = re.sub(r'\\u\w\w\w\w', '', review )
    # remove symbol, number, and strange char
    review = re.sub(r"[.,:;+!\-_<^/=?\"'\(\)\d\*]", " ", review )

    return review

def slang_changer(review):
    kamus_slangword = eval(open("slangwords2.txt").read())
    # search pola kata (contoh kpn -> kapan)
    pattern = re.compile(r'\b( ' + '|'.join(kamus_slangword.keys()) + r')\b')
    content = []
    for kata in review:
        # replace slangword berdasarkan pola review yg telah ditentukan
        filteredSlang = pattern.sub(lambda x: kamus_slangword[x.group()], kata)
        content.append(filteredSlang.lower())
    review = content
    return review

# Membuat set stopwords dari NLTK dan punctuation
sw_indo = set(stopwords.words('indonesian') + list(punctuation))

# Menambahkan stopwords dari stop_words dan stopwords tambahan sendiri
additional_stopwords = ["kata1", "kata2", "kata3"]
sw_indo.update(word for word in StopWordRemoverFactory().get_stop_words() if word not in sw_indo)
sw_indo.update(additional_stopwords)

sw_indo = stopwords.words('indonesian') + list(punctuation)
sw_indo = sw_indo + [i for i in StopWordRemoverFactory().get_stop_words() if i not in sw_indo]
print(len(sw_indo))

"""### Clean Distraction Letters"""

df['clean'] = df.review.apply(lambda x: cleaner(x))

df.head()

"""### Tokenize"""

df['tokenize'] = df.clean.apply(lambda x: word_tokenize(x))

df.head()

"""### Change slangword"""

df['slangwords'] = df.tokenize.apply(lambda x: slang_changer(x))

df.head()

"""### Inverse tokenize"""

df['invers'] = df.slangwords.apply(lambda x: ' '.join(x))

df.head()

from google.colab import files
df.to_csv('mandiri_done.csv', index=False)
files.download('mandiri_done.csv')

"""## Translate"""

!pip install googletrans==3.1.0a0

import pandas as pd
import googletrans
from googletrans import Translator

df_new = df[df["rating"] >= 3]
df_new.shape

translator = Translator()
df_new['invers'] = df_new['invers'].astype(str) #changing datatype to string
df_new['review_eng'] = df_new['invers'].apply(lambda x:translator.translate(x).text)
df_new

"""## Labeling"""

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

score = [analyzer.polarity_scores(x) for x in df_new['review_eng']]
df_new['negative_score'] = [x['neg'] for x in score]
df_new['neutral_score'] = [x['neu'] for x in score]
df_new['positive_score'] = [x['pos'] for x in score]
df_new['compound_score'] = [x['compound'] for x in score]

df_new['sentiment'] = df_new.compound_score.apply(lambda x: 'positive' if x>=0.05 else ('negative' if x<=-0.05 else 'neutral'))

df_new.head()

print('sentimen positive:', df_new[df_new.sentiment == 'positive'].shape[0])
print('sentimen neutral:', df_new[df_new.sentiment == 'neutral'].shape[0])
print('sentimen negative:', df_new[df_new.sentiment == 'negative'].shape[0])

# Save the result
from google.colab import files
df_new.to_csv('mandiri_done2.csv', index=False)
files.download('mandiri_done2.csv')

"""## concat review"""

df_new2 = df[df["rating"] < 3]
df_new.shape

df_new2['sentiment'] = df_new2.rating.apply(lambda x: 'neutral' if x==3 else 'negative')

df_result = pd.concat([df_new, df_new2], sort=False).sort_index()
df_result.head()

print('sentimen positive:', df_result[df_result.sentiment == 'positive'].shape[0])
print('sentimen neutral:', df_result[df_result.sentiment == 'neutral'].shape[0])
print('sentimen negative:', df_result[df_result.sentiment == 'negative'].shape[0])

# Save the result
from google.colab import files
df_result.to_csv('mandiri_done3.csv', index=False)
files.download('mandiri_done3.csv')

"""## Pre-processing (2)"""

# Importing dataset
import pandas as pd
df = pd.read_csv('mandiri_done3.csv')

# create function
def stopword_changer(sentence):
    return [i for i in sentence if i not in sw_indo]

def stemmer_changer(sentence):
  return [StemmerFactory().create_stemmer().stem(i) for i in sentence]

df.review

df['tokenize2'] = df.invers.apply(lambda x: word_tokenize(str(x)) if pd.notnull(x) else [])

# delete stopword
df['stopword'] = df.tokenize2.apply(lambda x: stopword_changer(x))

df.stopword

# stemming
df['stemming'] = df.stopword.apply(lambda x: stemmer_changer(x))

# invers token
df['invers2'] = df.stemming.apply(lambda x: ' '.join(x))

# save
from google.colab import files
df.to_csv('mandiri_ready.csv', index=False)
files.download('mandiri_ready.csv')



"""# **Exploratory Data Analysis**

### rasio sentimen
"""

# install package
import numpy as np
import pandas as pd
# For visualizations
import matplotlib.pyplot as plt
# For regular expressions
import re
# For handling string
import string
# For performing mathematical operations
import math

print('sentimen positif:', df[df.sentiment == 'positive'].shape[0])
print('sentimen netral:', df[df.sentiment == 'neutral'].shape[0])
print('sentimen negatif:', df[df.sentiment == 'negative'].shape[0])

import matplotlib.pyplot as plt

# Setting labels
labels = ['NEGATIVE', 'POSITIVE', 'NEUTRAL']

# Setting value
# Add the count for the 'neutral' sentiment
data = [
    df[df.sentiment == 'negative'].shape[0],
    df[df.sentiment == 'positive'].shape[0],
    df[df.sentiment == 'neutral'].shape[0]
]

# colors
colors = ['#FD7702', '#003366', '#808080']  # Added color for neutral

# explosion
explode = (0.05, 0.05, 0.05)  # Added explode for neutral

# Pie Chart
plt.pie(data, colors=colors, labels=labels,
        autopct='%1.1f%%', pctdistance=0.85, explode=explode)

# draw circle
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()

# Adding Circle in Pie chart
fig.gca().add_artist(centre_circle)

# Adding Title of chart
plt.title('SENTIMEN ANALYSIS')

# Displaying Chart
plt.show()

"""### sentimen overtime"""

df = pd.read_csv('/content/mandiri_ready.csv')

import pandas as pd
import matplotlib.pyplot as plt

# Misalkan df adalah DataFrame Anda
# Pastikan kolom 'date' sudah dalam format datetime
df['date'] = pd.to_datetime(df['date'])

# Ekstrak bulan dari kolom 'date'
df['month'] = df['date'].dt.to_period('M')

# Kelompokkan data berdasarkan bulan dan sentimen
grouped_data = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)

# Gambarkan grafik garis
plt.figure(figsize=(20, 8))
ax = grouped_data[['positive', 'negative', 'neutral']].plot(kind='line', marker='o', linestyle='-', color=['#003366', '#FD7702', '#808080'])

# Menambahkan label pada setiap titik data
for sentiment in ['positive', 'negative', 'neutral']:
    for i, value in enumerate(grouped_data[sentiment]):
        ax.annotate(f'{value}',  # Label value
                    (grouped_data.index[i], value),  # Point location
                    textcoords="offset points",
                    xytext=(0,5),  # Offset position
                    ha='center',
                    fontsize=8,  # Font size for clarity
                    bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))  # Background for label

# Konfigurasi tambahan untuk plot
plt.title('Sentiment Over Time')
plt.xlabel('Month')
plt.ylabel('Number of Reviews')
plt.legend(title='Sentiment', loc='upper right')

# Menampilkan grafik
plt.tight_layout()  # Adjust layout to ensure everything fits
plt.show()

import pandas as pd

# Misalkan df adalah DataFrame Anda
# Pastikan kolom 'date' sudah dalam format datetime
df['date'] = pd.to_datetime(df['date'])

# Ekstrak bulan dari kolom 'date'
df['month'] = df['date'].dt.to_period('M')

# Kelompokkan data berdasarkan bulan dan sentimen
grouped_data = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)

# Simpan tabel ke dalam file CSV
grouped_data.to_csv('sentiment_per_monthmandiri.csv', index=True)

print("Tabel berhasil disimpan ke dalam file sentiment_per_month.csv")

"""## Topic modelling

### TF-IDF
"""

import pandas as pd
Data = pd.read_csv('/content/mandiri_ready.csv')

from gensim import corpora
from gensim.models import TfidfModel, LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Praproses teks
documents = Data.loc[Data['sentiment'] == 'negative', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus_bow = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model TF-IDF
tfidf_model = TfidfModel(corpus_bow)
corpus_tfidf = tfidf_model[corpus_bow]

# Membangun model LDA dengan representasi TF-IDF
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

"""### BOW"""

from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Preproses teks
documents = Data.loc[Data['sentiment'] == 'negative', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model LDA
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

"""## Top word"""

Data['sentiment'].value_counts()

Data_All = Data['invers2']
Data_Negatif = Data[Data['sentiment'] == 'negative']['invers2']
Data_Netral = Data[Data['sentiment'] == 'neutral']['invers2']
Data_Positif = Data[Data['sentiment'] == 'positive']['invers2']

# Python program to convert a list
# using list comprehension
data_split_all = ' '.join(map(str, Data_All))
data_split_negatif = ' '.join(map(str, Data_Negatif))
data_split_positif = ' '.join(map(str, Data_Positif))

# split() returns list of all the words in the string
split_it_all = data_split_all.split()
split_it_negatif = data_split_negatif.split()
split_it_positif = data_split_positif.split()

# Pass the split_it list to instance of Counter class.
Counter_all = Counter(split_it_all)
Counter_negatif = Counter(split_it_negatif)
Counter_positif = Counter(split_it_positif)

# most_common() produces k frequently encountered
# input values and their respective counts.
most_occur_all = Counter_all.most_common(10)
most_occur_negatif = Counter_negatif.most_common(10)
most_occur_positif = Counter_positif.most_common(10)

print(most_occur_all)
print(len(Counter_all))
print(most_occur_negatif)
print(len(Counter_negatif))
print(most_occur_positif)
print(len(Counter_positif))

# Sort words and frequencies for all data in ascending order
sorted_all = sorted(most_occur_all, key=lambda x: x[1])
words_all, frequencies_all = zip(*sorted_all)

# Create a horizontal bar chart for all data
plt.figure(figsize=(10, 6))
plt.barh(words_all, frequencies_all, color='blue')
plt.title('Top Words in All Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

# Sort words and frequencies for negative data in ascending order
sorted_negatif = sorted(most_occur_negatif, key=lambda x: x[1])
words_negatif, frequencies_negatif = zip(*sorted_negatif)

# Create a horizontal bar chart for negative data
plt.figure(figsize=(10, 6))
plt.barh(words_negatif, frequencies_negatif, color='red')
plt.title('Top Words in Negative Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

# Sort words and frequencies for positive data in ascending order
sorted_positif = sorted(most_occur_positif, key=lambda x: x[1])
words_positif, frequencies_positif = zip(*sorted_positif)

# Create a horizontal bar chart for positive data
plt.figure(figsize=(10, 6))
plt.barh(words_positif, frequencies_positif, color='green')
plt.title('Top Words in Positive Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

"""## Word Cloud"""

df_positive = Data.copy()[Data.sentiment == 'positive']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_positive.png')
plt.show()

df_positive = Data.copy()[Data.sentiment == 'negative']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

df_positive = Data.copy()[Data.sentiment == 'neutral']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

"""# **BNI**

---

# **SCRAPING BNI**

### PLAYSTORE
"""

# install package
!pip install google-play-scraper
!pip install app_store_scraper

# Import all library
from google_play_scraper import Sort, reviews_all
from app_store_scraper import AppStore
import pandas as pd
import numpy as np
from google.colab import files
from datetime import date

from google_play_scraper import Sort, reviews_all
import pandas as pd
from datetime import datetime

# Define the search keywords
keywords = [
    "cybersecurity", "siber", "keamanan", "hacking", "fraud", "protection", "breach","kebocoran", "ancaman",
    "serangan", "phishing", "malware", "vulnerabilitas", "insiden", "pencurian", "rekening dibobol", "rekening hack",
    "rekening diretas", "transaksi tidak sah", "transaksi mencurigakan", "transaksi tidak dikenal", "saldo hilang",
    "uang raib", "uang hilang", "saldo berkurang", "penipuan online", "pelanggaran keamanan", "akun dibobol",
    "kejahatan siber", "pelanggaran keamanan", "kecurangan", "serangan", "cyber attack", "skimming", "pembobolan",
    "identity theft", "financial loss", "unauthorized access", "data leak", "account theft", "account takeover",
    "digital fraud", "data privasi", "data pribadi", "identitas", "serangan siber"
]

# Define the date range
start_date = datetime(2023, 6, 30)
end_date = datetime(2024, 6, 30)

# Scrape all reviews for the app
reviews = reviews_all(
    'src.com.bni',
    sleep_milliseconds=0,  # Adjust this value if needed to avoid overloading the server
    lang='id',  # Set the language to Indonesian
    country='id',  # Set the country to Indonesia
    sort=Sort.NEWEST  # Sort the reviews by newest first
)

# Convert to DataFrame
df = pd.DataFrame(reviews)

# Convert the date field to datetime
df['at'] = pd.to_datetime(df['at'])

# Filter reviews based on keywords
keyword_pattern = '|'.join(keywords)
filtered_by_keywords = df[df['content'].str.contains(keyword_pattern, case=False, na=False)]

# Filter reviews based on date range
filtered_reviews = filtered_by_keywords[(filtered_by_keywords['at'] >= start_date) & (filtered_by_keywords['at'] <= end_date)]

print(filtered_reviews.shape)
print(filtered_reviews.head())

# Save the DataFrame to a CSV file
filtered_reviews.to_csv('bni_playstore.csv', encoding='utf-8-sig')

# Download the CSV file
files.download('bni_playstore.csv')

"""# **PROSES**

### cek sentimen
"""

df = pd.read_csv('/content/bni_playstore.csv')
# drop kolom yang engga penting
df.drop(columns={'Unnamed: 0','reviewId', 'userName', 'userImage', 'thumbsUpCount', 'reviewCreatedVersion', 'replyContent','repliedAt','appVersion'},inplace = True)
df.rename(columns= {'score': 'rating', 'content': 'review', 'at': 'date'},inplace = True)
df.insert(loc=0, column='source', value='Google Play')

df['source'].value_counts()['Google Play']

print('sentimen positif:', df[df.rating > 3].shape[0])
print('sentimen netral:', df[df.rating == 3].shape[0])
print('sentimen negatif:', df[df.rating < 3].shape[0])

"""### import package"""

# install package
!pip install Sastrawi

# Import library
import re
import nltk
import pickle

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from wordcloud import WordCloud
from collections import Counter
from string import punctuation
from IPython.display import Math

from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
nltk.download('stopwords')
nltk.download('punkt')

!pip install Sastrawi
from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory
from Sastrawi.Stemmer.StemmerFactory import StemmerFactory

!pip install jcopml
from jcopml.tuning import random_search_params as rsp

"""## Pre-processing data

### Create Text Processing Function
"""

def cleaner(sentence):
    review = sentence.lower()

    # clear double letters
    review = re.compile(r"(.)\1{1,}").sub(r"\1", review )
    # clear @username
    review = re.sub('@[^\s]+', '', review )
    # clear #tag
    review = re.sub(r'#([^\s]+)', '', review )
    # clear non-ascii value
    review = re.sub(r'[^\x00-\x7f]', r'', review )
    review = re.sub(r'(\\u[0-9A-Fa-f]+)', r'', review )
    review = re.sub(r"[^A-Za-z0-9^,!.\/'+-=]", " ", review )
    review = re.sub(r'\\u\w\w\w\w', '', review )
    # remove symbol, number, and strange char
    review = re.sub(r"[.,:;+!\-_<^/=?\"'\(\)\d\*]", " ", review )

    return review

def slang_changer(review):
    kamus_slangword = eval(open("slangwords2.txt").read())
    # search pola kata (contoh kpn -> kapan)
    pattern = re.compile(r'\b( ' + '|'.join(kamus_slangword.keys()) + r')\b')
    content = []
    for kata in review:
        # replace slangword berdasarkan pola review yg telah ditentukan
        filteredSlang = pattern.sub(lambda x: kamus_slangword[x.group()], kata)
        content.append(filteredSlang.lower())
    review = content
    return review

# Membuat set stopwords dari NLTK dan punctuation
sw_indo = set(stopwords.words('indonesian') + list(punctuation))

# Menambahkan stopwords dari stop_words dan stopwords tambahan sendiri
additional_stopwords = ["kata1", "kata2", "kata3"]
sw_indo.update(word for word in StopWordRemoverFactory().get_stop_words() if word not in sw_indo)
sw_indo.update(additional_stopwords)

sw_indo = stopwords.words('indonesian') + list(punctuation)
sw_indo = sw_indo + [i for i in StopWordRemoverFactory().get_stop_words() if i not in sw_indo]
print(len(sw_indo))

"""### Clean Distraction Letters"""

df['clean'] = df.review.apply(lambda x: cleaner(x))

df.head()

"""### Tokenize"""

df['tokenize'] = df.clean.apply(lambda x: word_tokenize(x))

df.head()

"""### Change slangword"""

df['slangwords'] = df.tokenize.apply(lambda x: slang_changer(x))

df.head()

"""### Inverse tokenize"""

df['invers'] = df.slangwords.apply(lambda x: ' '.join(x))

df.head()

from google.colab import files
df.to_csv('bni_done.csv', index=False)
files.download('bni_done.csv')

"""## Translate"""

!pip install googletrans==3.1.0a0

import pandas as pd
import googletrans
from googletrans import Translator

df_new = df[df["rating"] >= 3]
df_new.shape

translator = Translator()
df_new['invers'] = df_new['invers'].astype(str) #changing datatype to string
df_new['review_eng'] = df_new['invers'].apply(lambda x:translator.translate(x).text)
df_new

"""## Labeling"""

!pip install vaderSentiment

from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
analyzer = SentimentIntensityAnalyzer()

score = [analyzer.polarity_scores(x) for x in df_new['review_eng']]
df_new['negative_score'] = [x['neg'] for x in score]
df_new['neutral_score'] = [x['neu'] for x in score]
df_new['positive_score'] = [x['pos'] for x in score]
df_new['compound_score'] = [x['compound'] for x in score]

df_new['sentiment'] = df_new.compound_score.apply(lambda x: 'positive' if x>=0.05 else ('negative' if x<=-0.05 else 'neutral'))

df_new.head()

print('sentimen positive:', df_new[df_new.sentiment == 'positive'].shape[0])
print('sentimen neutral:', df_new[df_new.sentiment == 'neutral'].shape[0])
print('sentimen negative:', df_new[df_new.sentiment == 'negative'].shape[0])

# Save the result
from google.colab import files
df_new.to_csv('bni_done2.csv', index=False)
files.download('bni_done2.csv')

"""## concat review"""

df_new2 = df[df["rating"] < 3]
df_new.shape

df_new2['sentiment'] = df_new2.rating.apply(lambda x: 'neutral' if x==3 else 'negative')

df_result = pd.concat([df_new, df_new2], sort=False).sort_index()
df_result.head()

print('sentimen positive:', df_result[df_result.sentiment == 'positive'].shape[0])
print('sentimen neutral:', df_result[df_result.sentiment == 'neutral'].shape[0])
print('sentimen negative:', df_result[df_result.sentiment == 'negative'].shape[0])

# Save the result
from google.colab import files
df_result.to_csv('bni_done3.csv', index=False)
files.download('bni_done3.csv')

"""## Pre-processing (2)"""

# Importing dataset
import pandas as pd
df = pd.read_csv('bni_done3.csv')

# create function
def stopword_changer(sentence):
    return [i for i in sentence if i not in sw_indo]

def stemmer_changer(sentence):
  return [StemmerFactory().create_stemmer().stem(i) for i in sentence]

df.review

df['tokenize2'] = df.invers.apply(lambda x: word_tokenize(str(x)) if pd.notnull(x) else [])

# delete stopword
df['stopword'] = df.tokenize2.apply(lambda x: stopword_changer(x))

df.stopword

# stemming
df['stemming'] = df.stopword.apply(lambda x: stemmer_changer(x))

# invers token
df['invers2'] = df.stemming.apply(lambda x: ' '.join(x))

# save
from google.colab import files
df.to_csv('bni_ready.csv', index=False)
files.download('bni_ready.csv')



"""# **Exploratory Data Analysis**"""

df = pd.read_csv('/content/bni_ready.csv')

"""### rasio sentimen"""

df = pd.read_csv('/content/bni_playstore.csv')

# install package
import numpy as np
import pandas as pd
# For visualizations
import matplotlib.pyplot as plt
# For regular expressions
import re
# For handling string
import string
# For performing mathematical operations
import math

print('sentimen positif:', df[df.sentiment == 'positive'].shape[0])
print('sentimen netral:', df[df.sentiment == 'neutral'].shape[0])
print('sentimen negatif:', df[df.sentiment == 'negative'].shape[0])

import matplotlib.pyplot as plt

# Setting labels
labels = ['NEGATIVE', 'POSITIVE', 'NEUTRAL']

# Setting value
# Add the count for the 'neutral' sentiment
data = [
    df[df.sentiment == 'negative'].shape[0],
    df[df.sentiment == 'positive'].shape[0],
    df[df.sentiment == 'neutral'].shape[0]
]

# colors
colors = ['#FD7702', '#003366', '#808080']  # Added color for neutral

# explosion
explode = (0.05, 0.05, 0.05)  # Added explode for neutral

# Pie Chart
plt.pie(data, colors=colors, labels=labels,
        autopct='%1.1f%%', pctdistance=0.85, explode=explode)

# draw circle
centre_circle = plt.Circle((0, 0), 0.70, fc='white')
fig = plt.gcf()

# Adding Circle in Pie chart
fig.gca().add_artist(centre_circle)

# Adding Title of chart
plt.title('SENTIMEN ANALYSIS')

# Displaying Chart
plt.show()

"""### sentimen overtime"""

import pandas as pd
import matplotlib.pyplot as plt

# Misalkan df adalah DataFrame Anda
# Pastikan kolom 'date' sudah dalam format datetime
df['date'] = pd.to_datetime(df['date'])

# Ekstrak bulan dari kolom 'date'
df['month'] = df['date'].dt.to_period('M')

# Kelompokkan data berdasarkan bulan dan sentimen
grouped_data = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)

# Gambarkan grafik garis
plt.figure(figsize=(20, 8))
ax = grouped_data[['positive', 'negative', 'neutral']].plot(kind='line', marker='o', linestyle='-', color=['#003366', '#FD7702', '#808080'])

# Menambahkan label pada setiap titik data
for sentiment in ['positive', 'negative', 'neutral']:
    for i, value in enumerate(grouped_data[sentiment]):
        ax.annotate(f'{value}',  # Label value
                    (grouped_data.index[i], value),  # Point location
                    textcoords="offset points",
                    xytext=(0,5),  # Offset position
                    ha='center',
                    fontsize=8,  # Font size for clarity
                    bbox=dict(facecolor='white', alpha=0.6, edgecolor='none'))  # Background for label

# Konfigurasi tambahan untuk plot
plt.title('Sentiment Over Time')
plt.xlabel('Month')
plt.ylabel('Number of Reviews')
plt.legend(title='Sentiment', loc='upper right')

# Menampilkan grafik
plt.tight_layout()  # Adjust layout to ensure everything fits
plt.show()

import pandas as pd

# Misalkan df adalah DataFrame Anda
# Pastikan kolom 'date' sudah dalam format datetime
df['date'] = pd.to_datetime(df['date'])

# Ekstrak bulan dari kolom 'date'
df['month'] = df['date'].dt.to_period('M')

# Kelompokkan data berdasarkan bulan dan sentimen
grouped_data = df.groupby(['month', 'sentiment']).size().unstack(fill_value=0)

# Simpan tabel ke dalam file CSV
grouped_data.to_csv('sentiment_per_monthbni.csv', index=True)

print("Tabel berhasil disimpan ke dalam file sentiment_per_month.csv")

"""## Topic modelling

### TF-IDF
"""

import pandas as pd
Data = pd.read_csv('/content/bni_ready.csv')

from gensim import corpora
from gensim.models import TfidfModel, LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Praproses teks
documents = Data.loc[Data['sentiment'] == 'negative', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus_bow = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model TF-IDF
tfidf_model = TfidfModel(corpus_bow)
corpus_tfidf = tfidf_model[corpus_bow]

# Membangun model LDA dengan representasi TF-IDF
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus_tfidf, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

"""### BOW"""

from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Preproses teks
documents = Data.loc[Data['sentiment'] == 'negative', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model LDA
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)

"""## Top word"""

Data['sentiment'].value_counts()

Data_All = Data['invers2']
Data_Negatif = Data[Data['sentiment'] == 'negative']['invers2']
Data_Netral = Data[Data['sentiment'] == 'neutral']['invers2']
Data_Positif = Data[Data['sentiment'] == 'positive']['invers2']

# Python program to convert a list
# using list comprehension
data_split_all = ' '.join(map(str, Data_All))
data_split_negatif = ' '.join(map(str, Data_Negatif))
data_split_positif = ' '.join(map(str, Data_Positif))

# split() returns list of all the words in the string
split_it_all = data_split_all.split()
split_it_negatif = data_split_negatif.split()
split_it_positif = data_split_positif.split()

# Pass the split_it list to instance of Counter class.
Counter_all = Counter(split_it_all)
Counter_negatif = Counter(split_it_negatif)
Counter_positif = Counter(split_it_positif)

# most_common() produces k frequently encountered
# input values and their respective counts.
most_occur_all = Counter_all.most_common(10)
most_occur_negatif = Counter_negatif.most_common(10)
most_occur_positif = Counter_positif.most_common(10)

print(most_occur_all)
print(len(Counter_all))
print(most_occur_negatif)
print(len(Counter_negatif))
print(most_occur_positif)
print(len(Counter_positif))

# Sort words and frequencies for all data in ascending order
sorted_all = sorted(most_occur_all, key=lambda x: x[1])
words_all, frequencies_all = zip(*sorted_all)

# Create a horizontal bar chart for all data
plt.figure(figsize=(10, 6))
plt.barh(words_all, frequencies_all, color='blue')
plt.title('Top Words in All Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

# Sort words and frequencies for negative data in ascending order
sorted_negatif = sorted(most_occur_negatif, key=lambda x: x[1])
words_negatif, frequencies_negatif = zip(*sorted_negatif)

# Create a horizontal bar chart for negative data
plt.figure(figsize=(10, 6))
plt.barh(words_negatif, frequencies_negatif, color='red')
plt.title('Top Words in Negative Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

# Sort words and frequencies for positive data in ascending order
sorted_positif = sorted(most_occur_positif, key=lambda x: x[1])
words_positif, frequencies_positif = zip(*sorted_positif)

# Create a horizontal bar chart for positive data
plt.figure(figsize=(10, 6))
plt.barh(words_positif, frequencies_positif, color='green')
plt.title('Top Words in Positive Data')
plt.xlabel('Frequency')
plt.ylabel('Words')
plt.tight_layout()
plt.show()

"""## Word Cloud"""

df_positive = Data.copy()[Data.sentiment == 'positive']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_positive.png')
plt.show()

df_positive = Data.copy()[Data.sentiment == 'negative']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

df_positive = Data.copy()[Data.sentiment == 'neutral']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

"""# **SUMMARY**"""

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Data
data = {
    'Bank': ['BRI', 'BCA', 'Mandiri', 'BNI'],
    'Positif': [413, 69, 73, 50],
    'Netral': [4, 10, 6, 2],
    'Negatif': [295, 141, 160, 104]
}

# Membuat DataFrame
df = pd.DataFrame(data)

# Mengatur DataFrame untuk visualisasi
df_melted = df.melt(id_vars='Bank', var_name='Sentimen', value_name='Jumlah')

# Mengatur ukuran grafik
plt.figure(figsize=(10, 6))

# Membuat bar chart
sns.barplot(x='Bank', y='Jumlah', hue='Sentimen', data=df_melted, palette='viridis')

# Menambahkan label dan judul
plt.xlabel('Bank')
plt.ylabel('Jumlah')
plt.title('Perbandingan Sentimen Pelanggan Berdasarkan Bank')
plt.legend(title='Sentimen')

# Menampilkan grafik
plt.show()

import seaborn as sns

# Data
heatmap_data = df.set_index('Bank')

# Membuat Heatmap
plt.figure(figsize=(8, 6))
sns.heatmap(heatmap_data, annot=True, cmap='YlGnBu', fmt='d')

# Menambahkan label dan judul
plt.title('Heatmap Sentimen Pelanggan Berdasarkan Bank')
plt.xlabel('Sentimen')
plt.ylabel('Bank')

# Menampilkan grafik
plt.show()

# Membuat Stacked Bar Chart
df.set_index('Bank').plot(kind='bar', stacked=True, figsize=(10, 6), color=['#4caf50', '#ffeb3b', '#f44336'])

# Menambahkan label dan judul
plt.xlabel('Bank')
plt.ylabel('Jumlah')
plt.title('Distribusi Sentimen Pelanggan Berdasarkan Bank')
plt.legend(title='Sentimen')

# Menampilkan grafik
plt.show()

# Data untuk Pie Chart
labels = ['Positif', 'Netral', 'Negatif']
colors = ['#4caf50', '#ffeb3b', '#f44336']

fig, axes = plt.subplots(2, 2, figsize=(14, 10))
fig.suptitle('Proporsi Sentimen Pelanggan untuk Setiap Bank', fontsize=16)

for ax, bank in zip(axes.flatten(), df['Bank']):
    sizes = df[df['Bank'] == bank][['Positif', 'Netral', 'Negatif']].values[0]
    ax.pie(sizes, labels=labels, autopct='%1.1f%%', startangle=140, colors=colors)
    ax.set_title(bank)

plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# Contoh data tren sentimen dari waktu ke waktu
data = {
    'Tanggal': pd.date_range(start='2023-01-01', periods=12, freq='M'),
    'BRI_Positif': [30, 45, 50, 60, 70, 80, 85, 90, 95, 100, 105, 110],
    'BRI_Netral': [5, 6, 4, 3, 2, 4, 3, 5, 4, 6, 5, 4],
    'BRI_Negatif': [20, 18, 17, 16, 15, 14, 13, 12, 11, 10, 9, 8],
    'BCA_Positif': [10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65],
    'BCA_Netral': [8, 7, 6, 5, 4, 5, 6, 7, 8, 7, 6, 5],
    'BCA_Negatif': [15, 14, 13, 12, 11, 10, 9, 8, 7, 6, 5, 4],
    'Mandiri_Positif': [12, 17, 22, 27, 32, 37, 42, 47, 52, 57, 62, 67],
    'Mandiri_Netral': [6, 5, 4, 5, 6, 5, 4, 3, 4, 5, 6, 5],
    'Mandiri_Negatif': [13, 12, 11, 10, 9, 8, 7, 6, 5, 4, 3, 2],
    'BNI_Positif': [8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30],
    'BNI_Netral': [4, 5, 4, 3, 2, 3, 4, 5, 4, 3, 2, 1],
    'BNI_Negatif': [11, 10, 9, 8, 7, 6, 5, 4, 3, 2, 1, 0]
}

# Membuat DataFrame
df = pd.DataFrame(data)

# Mengatur ukuran grafik
plt.figure(figsize=(14, 8))

# Membuat line chart untuk setiap sentimen dari masing-masing bank
for sentiment in ['Positif', 'Netral', 'Negatif']:
    for bank in ['BRI', 'BCA', 'Mandiri', 'BNI']:
        plt.plot(df['Tanggal'], df[f'{bank}_{sentiment}'], label=f'{bank} {sentiment}')

# Menambahkan label dan judul
plt.xlabel('Tanggal')
plt.ylabel('Jumlah Sentimen')
plt.title('Tren Sentimen Keseluruhan Berdasarkan Bank')
plt.legend(title='Sentimen')

# Menampilkan grafik
plt.show()

# Contoh Data
df_area = df[['Tanggal', 'BRI_Positif', 'BRI_Netral', 'BRI_Negatif']]

# Mengatur ukuran grafik
plt.figure(figsize=(14, 8))

# Membuat area chart
plt.stackplot(df_area['Tanggal'], df_area['BRI_Positif'], df_area['BRI_Netral'], df_area['BRI_Negatif'], labels=['Positif', 'Netral', 'Negatif'], colors=['#4caf50', '#ffeb3b', '#f44336'])

# Menambahkan label dan judul
plt.xlabel('Tanggal')
plt.ylabel('Jumlah Sentimen')
plt.title('Tren Sentimen Keseluruhan Bank BRI')
plt.legend(loc='upper left', title='Sentimen')

# Menampilkan grafik
plt.show()

import matplotlib.pyplot as plt

# Contoh data area dengan keluhan negatif
data = {
    'Area': ['Layanan Pelanggan', 'Keamanan Transaksi', 'Keterlambatan Transfer', 'Penggunaan Aplikasi', 'Respon Email'],
    'Jumlah Keluhan Negatif': [150, 200, 50, 100, 75]
}

# Membuat DataFrame
df = pd.DataFrame(data)

# Mengatur ukuran grafik
plt.figure(figsize=(10, 6))

# Membuat horizontal bar chart
plt.barh(df['Area'], df['Jumlah Keluhan Negatif'], color='#f44336')

# Menambahkan label dan judul
plt.xlabel('Jumlah Keluhan Negatif')
plt.ylabel('Area Layanan')
plt.title('Area dengan Keluhan Negatif Tertinggi')

# Menampilkan grafik
plt.show()

import matplotlib.pyplot as plt
import numpy as np

# Contoh data keluhan spesifik untuk masing-masing bank
data = {
    'Bank': ['BRI', 'BCA', 'Mandiri', 'BNI'],
    'Layanan Pelanggan': [150, 80, 60, 50],
    'Keamanan Transaksi': [200, 150, 100, 90],
    'Keterlambatan Transfer': [50, 20, 40, 30],
    'Penggunaan Aplikasi': [100, 50, 70, 40],
    'Respon Email': [75, 30, 50, 25]
}

# Membuat DataFrame
df = pd.DataFrame(data)

# Mengatur posisi batang
bar_width = 0.15
index = np.arange(len(df['Bank']))

# Mengatur ukuran grafik
plt.figure(figsize=(14, 8))

# Membuat grouped bar chart
plt.bar(index, df['Layanan Pelanggan'], bar_width, label='Layanan Pelanggan')
plt.bar(index + bar_width, df['Keamanan Transaksi'], bar_width, label='Keamanan Transaksi')
plt.bar(index + 2 * bar_width, df['Keterlambatan Transfer'], bar_width, label='Keterlambatan Transfer')
plt.bar(index + 3 * bar_width, df['Penggunaan Aplikasi'], bar_width, label='Penggunaan Aplikasi')
plt.bar(index + 4 * bar_width, df['Respon Email'], bar_width, label='Respon Email')

# Menambahkan label dan judul
plt.xlabel('Bank')
plt.ylabel('Jumlah Keluhan')
plt.title('Keluhan Spesifik Berdasarkan Bank')
plt.xticks(index + 2 * bar_width, df['Bank'])
plt.legend()

# Menampilkan grafik
plt.show()

import pandas as pd

# Nama file CSV yang akan digabungkan
file1 = 'brimo_ready.csv'
file2 = 'bca_ready.csv'
file3 = 'mandiri_ready.csv'

# Membaca file CSV menjadi DataFrame
df1 = pd.read_csv(file1)
df2 = pd.read_csv(file2)
df3 = pd.read_csv(file3)

# Menggabungkan DataFrame
Data = pd.concat([df1, df2, df3], ignore_index=True)

# Menyimpan DataFrame gabungan ke file CSV baru
Data.to_csv('combined_file.csv', index=False)

print("File CSV berhasil digabungkan dan disimpan sebagai 'combined_file.csv'")

"""## Word Cloud"""

df_positive = Data.copy()[Data.sentiment == 'positive']

pos_comment = ' '
for sentence in df_positive.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				colormap='seismic',
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_positive.png')
plt.show()

df_negative = Data.copy()[Data.sentiment == 'negative']

pos_comment = ' '
for sentence in df_negative.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				                  colormap = "seismic",
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_negative.png')
plt.show()

df_neutral = Data.copy()[Data.sentiment == 'neutral']

pos_comment = ' '
for sentence in df_neutral.invers2:
  pos_comment += str(sentence) + ' '
# print(pos_comment)

wordcloud = WordCloud(width = 800, height = 800,
				background_color ='white',
				max_words = 1000,
				                  colormap = "seismic",
				# stopwords = stopwords,
				min_font_size = 12).generate(pos_comment)

# plot the WordCloud image
plt.figure(figsize = (8, 8), facecolor = None)
plt.imshow(wordcloud)
plt.axis("off")
plt.tight_layout(pad = 0)

plt.savefig('wordcloud_neutral.png')
plt.show()

from gensim import corpora
from gensim.models import LdaModel
from gensim.parsing.preprocessing import preprocess_string

# Preproses teks
documents = Data.loc[Data['sentiment'] == 'positive', 'invers2'].dropna().astype(str).apply(lambda x: preprocess_string(x))

# Membangun representasi kata sebagai bag-of-words
dictionary = corpora.Dictionary(documents)
corpus = [dictionary.doc2bow(doc) for doc in documents]

# Membangun model LDA
num_topics = 10  # Ganti dengan jumlah topik yang diinginkan
lda_model = LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=15)

# Melihat topik dan kata-kata utama di masing-masing topik
topics = lda_model.print_topics(num_words=5)
for topic in topics:
    print(topic)